import streamlit as st
import asyncio
import os

# --- ã€é‡è¦ã€‘Streamlitã§éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã®ãŠã¾ã˜ãªã„ ---
try:
    asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

from langchain_community.document_loaders import CSVLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- è¨­å®š ---
st.set_page_config(page_title="æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ(Geminiç‰ˆ)", page_icon="ğŸŒ²")
st.title("ğŸŒ² æ£®æ—çµŒå–¶ãƒŠãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆ (Gemini 2.5)")

# APIã‚­ãƒ¼ã®å–å¾—
if "GOOGLE_API_KEY" not in st.session_state:
    st.session_state.GOOGLE_API_KEY = ""

if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google API Key", type="password")

if not api_key:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«Google APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.stop()

os.environ["GOOGLE_API_KEY"] = api_key

# --- RAGæ§‹ç¯‰ ---
@st.cache_resource
def build_vector_store():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    loader = CSVLoader(
        file_path="data/æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸.csv",
        encoding="utf-8",
        source_column="è³ªå• (Question)"
    )
    docs = loader.load()
    
    # Embeddingãƒ¢ãƒ‡ãƒ«
    # models/text-embedding-004 ã¯æœ€æ–°ã§æ­£ã—ã„ã§ã™
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

try:
    vectorstore = build_vector_store()
    # ã€ä¿®æ­£1ã€‘æ¤œç´¢æ•° k ã‚’ 3 -> 5 ã«å¢—ã‚„ã—ã¦å–ã‚Šã“ã¼ã—ã‚’é˜²ã
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© ---
prompt_template = """ã‚ãªãŸã¯æ£®æ—çµŒå–¶ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚ç…§æƒ…å ±ã€ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—å‚ç…§æƒ…å ±ã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

å‚ç…§æƒ…å ±:
{context}

è³ªå•:
{question}

å›ç­”:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# --- Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š ---
# ã€ä¿®æ­£2ã€‘ æ­£ã—ã„ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´
# gemini-2.5-flash ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚gemini-1.5-flash ã‚’æŒ‡å®šã—ã¾ã™ã€‚
# æ³¨æ„: LangChainã§ã¯ "models/" ã‚’ä»˜ã‘ãšã«æŒ‡å®šã™ã‚‹æ–¹ãŒå®‰å®šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# ã€ä¿®æ­£3ã€‘ return_source_documents=True ã‚’è¿½åŠ ã—ã¦ã€æ¤œç´¢çµæœã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True 
)

# --- ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Gemini 1.5 ãŒæ€è€ƒä¸­..."):
            try:
                # invokeã§å®Ÿè¡Œ
                response = qa_chain.invoke({"query": prompt})
                answer = response['result']
                source_docs = response['source_documents'] # æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

                st.markdown(answer)
                
                # ã€ä¿®æ­£4ã€‘ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä½•ãŒæ¤œç´¢ã•ã‚ŒãŸã‹ã‚’è¡¨ç¤º
                # ã“ã‚Œã§ã€Œãªãœæƒ…å ±ãŒãªã„ã¨è¨€ã‚ã‚ŒãŸã‹ã€ãŒã‚ã‹ã‚Šã¾ã™
                with st.expander("ğŸ” å‚ç…§ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã™ã‚‹"):
                    for i, doc in enumerate(source_docs):
                        st.markdown(f"**ãƒ©ãƒ³ã‚¯ {i+1}**")
                        st.text(doc.page_content)

                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")