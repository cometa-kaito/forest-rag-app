import streamlit as st
from langchain_community.document_loaders import CSVLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# --- è¨­å®š ---
st.set_page_config(page_title="æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ(Geminiç‰ˆ)", page_icon="ğŸŒ²")
st.title("ğŸŒ² æ£®æ—çµŒå–¶ãƒŠãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆ (Gemini)")

# APIã‚­ãƒ¼ã®å–å¾—ï¼ˆUIå…¥åŠ›ã¾ãŸã¯Secretsã‹ã‚‰ï¼‰
if "GOOGLE_API_KEY" not in st.session_state:
    st.session_state.GOOGLE_API_KEY = ""

# Streamlitã®Secretsã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã€ãªã‘ã‚Œã°ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å…¥åŠ›
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google API Key", type="password")

if not api_key:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«Google APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.stop()

# ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆ
os.environ["GOOGLE_API_KEY"] = api_key

# --- RAGæ§‹ç¯‰ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–ã—ã¦é«˜é€ŸåŒ–) ---
@st.cache_resource
def build_vector_store():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    loader = CSVLoader(
        file_path="data/æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸.csv",
        encoding="utf-8",
        source_column="è³ªå• (Question)"
    )
    docs = loader.load()
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆGeminiã®Embeddingãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

try:
    vectorstore = build_vector_store()
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© ---
prompt_template = """ã‚ãªãŸã¯æ£®æ—çµŒå–¶ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚ç…§æƒ…å ±ã€ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—å‚ç…§æƒ…å ±ã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

å‚ç…§æƒ…å ±:
{context}

è³ªå•:
{question}

å›ç­”:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# --- Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š ---
# gemini-1.5-flash ã¯é«˜é€Ÿã§ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„ãƒ¢ãƒ‡ãƒ«ã§ã™
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT}
)

# --- ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("GeminiãŒæ€è€ƒä¸­..."):
            try:
                response = qa_chain.invoke({"query": prompt})
                answer = response['result']
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")