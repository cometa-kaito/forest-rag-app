import streamlit as st
import asyncio
import os

# --- ã€é‡è¦ã€‘Streamlitã§éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã®ãŠã¾ã˜ãªã„ ---
try:
    asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

from langchain_community.document_loaders import CSVLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- è¨­å®š ---
st.set_page_config(page_title="æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ(Geminiç‰ˆ)", page_icon="ğŸŒ²")
st.title("ğŸŒ² æ£®æ—çµŒå–¶ãƒŠãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆ (Gemini 1.5)")

# APIã‚­ãƒ¼ã®å–å¾—
if "GOOGLE_API_KEY" not in st.session_state:
    st.session_state.GOOGLE_API_KEY = ""

if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google API Key", type="password")

if not api_key:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«Google APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.stop()

os.environ["GOOGLE_API_KEY"] = api_key

# --- RAGæ§‹ç¯‰ ---
@st.cache_resource
def build_vector_store():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    loader = CSVLoader(
        file_path="data/æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸.csv",
        encoding="utf-8",
        source_column="è³ªå• (Question)"
    )
    docs = loader.load()
    
    # ã€ä¿®æ­£1ã€‘ æ­£ã—ã„Embeddingãƒ¢ãƒ‡ãƒ«å (models/text-embedding-004)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

try:
    vectorstore = build_vector_store()
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© ---
prompt_template = """ã‚ãªãŸã¯æ£®æ—çµŒå–¶ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚ç…§æƒ…å ±ã€ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—å‚ç…§æƒ…å ±ã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

å‚ç…§æƒ…å ±:
{context}

è³ªå•:
{question}

å›ç­”:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# --- Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š ---
# ã€ä¿®æ­£2ã€‘ æ­£ã—ã„ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«å (gemini-1.5-flash)
# 2.5ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚1.5ãŒç¾åœ¨ã®æœ€æ–°é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT}
)

# --- ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Gemini 1.5 ãŒæ€è€ƒä¸­..."):
            try:
                response = qa_chain.invoke({"query": prompt})
                answer = response['result']
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")