import streamlit as st
from langchain_community.document_loaders import CSVLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# --- è¨­å®š ---
st.set_page_config(page_title="æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", page_icon="ğŸŒ²")
st.title("ğŸŒ² æ£®æ—çµŒå–¶ãƒŠãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆ")

# APIã‚­ãƒ¼ã®å–å¾—ï¼ˆStreamlit Secretsã‹ã‚‰èª­ã¿è¾¼ã‚€å®‰å…¨ãªæ–¹æ³•ï¼‰
# ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™å ´åˆã¯ .streamlit/secrets.toml ãŒå¿…è¦ã§ã™ãŒã€
# UIä¸Šã§å…¥åŠ›ã•ã›ã‚‹ç°¡æ˜“ç‰ˆã¨ã—ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""

api_key = st.sidebar.text_input("OpenAI API Key", type="password")

if not api_key:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.stop()
else:
    os.environ["OPENAI_API_KEY"] = api_key

# --- RAGæ§‹ç¯‰ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–ã—ã¦é«˜é€ŸåŒ–) ---
@st.cache_resource
def build_vector_store():
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    loader = CSVLoader(
        file_path="data/æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸.csv",
        encoding="utf-8",
        source_column="è³ªå• (Question)" # æ¤œç´¢ç²¾åº¦å‘ä¸Šã®ãŸã‚è³ªå•æ–‡ã‚’æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
    )
    docs = loader.load()
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ä¿å­˜
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

try:
    vectorstore = build_vector_store()
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© ---
prompt_template = """ã‚ãªãŸã¯æ£®æ—çµŒå–¶ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚ç…§æƒ…å ±ã€ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—å‚ç…§æƒ…å ±ã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

å‚ç…§æƒ…å ±:
{context}

è³ªå•:
{question}

å›ç­”:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# --- LLMã¨Chainã®å®šç¾© ---
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT}
)

# --- ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®å±¥æ­´ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIã®å›ç­”ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("è³‡æ–™ã‚’æ¤œç´¢ä¸­..."):
            try:
                # invokeã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’å–å¾—
                response = qa_chain.invoke({"query": prompt})
                answer = response['result']
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")