import streamlit as st
import asyncio
import os
import pandas as pd
from typing import List, Tuple

# --- ã€é‡è¦ã€‘Streamlitã§éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã®ãŠã¾ã˜ãªã„ ---
try:
    asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# --- è¨­å®š ---
st.set_page_config(page_title="æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ(Geminiç‰ˆ)", page_icon="ğŸŒ²", layout="wide")
st.title("ğŸŒ² æ£®æ—çµŒå–¶ãƒŠãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆ (Gemini 2.5)")

# APIã‚­ãƒ¼ã®å–å¾—
if "GOOGLE_API_KEY" not in st.session_state:
    st.session_state.GOOGLE_API_KEY = ""

if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google API Key", type="password")

if not api_key:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«Google APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.stop()

os.environ["GOOGLE_API_KEY"] = api_key


# --- ã€æ”¹å–„1ã€‘CSVã‚’ç›´æ¥èª­ã¿è¾¼ã¿ã€è³ªå•ã®ã¿ã‚’Embeddingå¯¾è±¡ã«ã™ã‚‹ ---
@st.cache_resource
def build_vector_store():
    # CSVã‚’ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆpandasã§åˆ¶å¾¡ã—ã‚„ã™ãã™ã‚‹ï¼‰
    df = pd.read_csv("data/æ£®æ—ãƒŠãƒ¬ãƒƒã‚¸.csv", encoding="utf-8-sig")
    
    st.sidebar.success(f"ğŸ“š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}ä»¶")
    
    # ã€é‡è¦ã€‘è³ªå•æ–‡ã®ã¿ã‚’page_contentã¨ã—ã€å›ç­”ã¨ã‚«ãƒ†ã‚´ãƒªã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«æ ¼ç´
    docs = []
    for idx, row in df.iterrows():
        doc = Document(
            page_content=row["è³ªå• (Question)"],  # è³ªå•ã®ã¿ã‚’Embeddingå¯¾è±¡
            metadata={
                "category": row["ã‚«ãƒ†ã‚´ãƒª"],
                "question": row["è³ªå• (Question)"],
                "answer": row["å›ç­” (Answer)"],
                "index": idx
            }
        )
        docs.append(doc)
    
    with st.sidebar.expander("èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5ä»¶ï¼‰"):
        for i, doc in enumerate(docs[:5]):
            st.text(f"{i+1}. {doc.page_content[:50]}...")
    
    # Embeddingãƒ¢ãƒ‡ãƒ«
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore, df


# --- ã€æ”¹å–„2ã€‘BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’è¿½åŠ  ---
def keyword_search(query: str, df: pd.DataFrame, top_k: int = 5) -> List[Tuple[int, float]]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢
    è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ä¸¡æ–¹ã‚’æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
    """
    from collections import Counter
    import re
    
    # ã‚¯ã‚¨ãƒªã‚’å˜èªã«åˆ†å‰²ï¼ˆæ—¥æœ¬èªå¯¾å¿œã®ç°¡æ˜“ç‰ˆï¼‰
    query_terms = set(re.findall(r'\w+', query.lower()))
    
    scores = []
    for idx, row in df.iterrows():
        # è³ªå•ã¨å›ç­”ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        text = f"{row['è³ªå• (Question)']} {row['å›ç­” (Answer)']}".lower()
        text_terms = set(re.findall(r'\w+', text))
        
        # ãƒãƒƒãƒã™ã‚‹å˜èªæ•°ã‚’ã‚¹ã‚³ã‚¢ã«
        match_count = len(query_terms & text_terms)
        if match_count > 0:
            scores.append((idx, match_count))
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]


# --- ã€æ”¹å–„3ã€‘ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè£… ---
def hybrid_search(query: str, vectorstore, df: pd.DataFrame, k: int = 5) -> List[dict]:
    """
    ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
    """
    # 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
    vector_results = vectorstore.similarity_search_with_score(query, k=k)
    
    # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    keyword_results = keyword_search(query, df, top_k=k)
    
    # 3. ã‚¹ã‚³ã‚¢ã‚’çµ±åˆï¼ˆRRF: Reciprocal Rank Fusionï¼‰
    combined_scores = {}
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢è¿½åŠ 
    for rank, (doc, score) in enumerate(vector_results):
        idx = doc.metadata.get("index")
        if idx is not None:
            # FAISSã®ã‚¹ã‚³ã‚¢ã¯è·é›¢ãªã®ã§ã€å°ã•ã„ã»ã©è‰¯ã„ â†’ å¤‰æ›
            combined_scores[idx] = combined_scores.get(idx, 0) + 1 / (rank + 1)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢è¿½åŠ ï¼ˆé‡ã¿ä»˜ã‘ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã‚’é‡è¦–ï¼‰
    for rank, (idx, _) in enumerate(keyword_results):
        combined_scores[idx] = combined_scores.get(idx, 0) + 1.5 / (rank + 1)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«1.5å€ã®é‡ã¿
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]
    
    # çµæœã‚’æ•´å½¢
    results = []
    for idx, score in sorted_results:
        row = df.iloc[idx]
        results.append({
            "category": row["ã‚«ãƒ†ã‚´ãƒª"],
            "question": row["è³ªå• (Question)"],
            "answer": row["å›ç­” (Answer)"],
            "score": score
        })
    
    return results


try:
    vectorstore, df = build_vector_store()
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()


# --- Geminiãƒ¢ãƒ‡ãƒ«ã®è¨­å®š ---
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)


# --- ã€æ”¹å–„4ã€‘æ¤œç´¢çµæœã‚’ä½¿ã£ãŸå›ç­”ç”Ÿæˆ ---
def generate_answer(query: str, search_results: List[dict]) -> str:
    """æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ"""
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
    context_parts = []
    for i, result in enumerate(search_results):
        context_parts.append(
            f"ã€æƒ…å ±{i+1}ã€‘\n"
            f"ã‚«ãƒ†ã‚´ãƒª: {result['category']}\n"
            f"è³ªå•: {result['question']}\n"
            f"å›ç­”: {result['answer']}"
        )
    context = "\n\n".join(context_parts)
    
    prompt = f"""ã‚ãªãŸã¯æ£®æ—çµŒå–¶ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚ç…§æƒ…å ±ã€ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªãƒ«ãƒ¼ãƒ«ã€‘
1. å‚ç…§æƒ…å ±ã«ç›´æ¥é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚‹å ´åˆã¯ã€ãã®æƒ…å ±ã‚’å…ƒã«å›ç­”ã—ã¦ãã ã•ã„
2. å‚ç…§æƒ…å ±ã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ãã®æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„
3. å›ç­”ã¯ç°¡æ½”ã«ã€ã—ã‹ã—å¿…è¦ãªæƒ…å ±ã¯æ¼ã‚‰ã•ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„

å‚ç…§æƒ…å ±:
{context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:
{query}

å›ç­”:"""
    
    response = llm.invoke(prompt)
    return response.content


# --- ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Gemini 2.5 ãŒæ€è€ƒä¸­..."):
            try:
                # ã€æ”¹å–„ã€‘ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ
                search_results = hybrid_search(prompt, vectorstore, df, k=5)
                
                # å›ç­”ã‚’ç”Ÿæˆ
                answer = generate_answer(prompt, search_results)
                
                st.markdown(answer)
                
                # å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã‚¨ãƒªã‚¢ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
                with st.expander("ğŸ” å‚ç…§ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã™ã‚‹"):
                    for i, result in enumerate(search_results):
                        st.markdown(f"**ãƒ©ãƒ³ã‚¯ {i+1}** (ã‚¹ã‚³ã‚¢: {result['score']:.2f})")
                        st.markdown(f"- **ã‚«ãƒ†ã‚´ãƒª**: {result['category']}")
                        st.markdown(f"- **è³ªå•**: {result['question']}")
                        st.markdown(f"- **å›ç­”**: {result['answer']}")
                        st.divider()

                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.code(traceback.format_exc())